{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from surprise.model_selection import KFold\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## data 스크래이핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name1=['김밥천국','포보스','삼백집','밥뜨랑','다원','한려수도','봉태민','종로김밥','명동칼국수','돈토']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "def get_request_url(url) :\n",
    "    client_id = 'id'\n",
    "    client_secret= 'Secret'\n",
    "    rep = urllib.request.Request(url)\n",
    "    rep.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    rep.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "    try :\n",
    "        response = urllib.request.urlopen(rep)\n",
    "        if response.getcode() ==  200 :\n",
    "            print(\"[%s] Url request success\" % datetime.datetime.now())\n",
    "            return response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"[%s]Error for URL : %s\" % (datetime.datetime.now(), url))\n",
    "        return None\n",
    "\n",
    "    \n",
    "def getNaverSearchResult(sNode, search_text, page_start, display) :\n",
    "    base = \"https://openapi.naver.com/v1/search\"\n",
    "    node = \"/%s.json\" % (sNode)\n",
    "    params = \"?query=%s&start=%s&display=%s\" % (urllib.parse.quote(search_text),\n",
    "                                               page_start, display)\n",
    "    url = base+node+params\n",
    "    getData = get_request_url(url)\n",
    "    if getData == None :\n",
    "        return None\n",
    "    else :\n",
    "        return json.loads(getData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#검색결과 json으로 저장하기 \n",
    "# 원하는 검색 종류에 따라 형식 달라서\n",
    "#getPostData 함수의 수정이 필요\n",
    "# 이거는 blog 내용\n",
    "\n",
    "def getPostData(post, jsonResult) :\n",
    "    title = post['title']\n",
    "    description = post['description']\n",
    "    link = post['link']\n",
    "    bloggername=post['bloggername']\n",
    "    bloggerlink=post['bloggerlink']\n",
    "    \n",
    "    \n",
    "    jsonResult.append({'title' : title, 'description' : description,\n",
    "                      'link' : link, 'blogname' : bloggername,\n",
    "                       'bloggerlink' : bloggerlink})\n",
    "    return\n",
    "\n",
    "def main(search_text) :\n",
    "    jsonResult = [] #초기화\n",
    "    \n",
    "    #내 개인 설정 \n",
    "    sNode = 'blog'    #'news', 'blog', 'cafearticle'\n",
    "    name_len = len(name1)\n",
    "    display_count = 100\n",
    "    \n",
    "    jsonSearch = getNaverSearchResult(sNode, search_text, name_len , display_count)\n",
    "    \n",
    "    while ((jsonSearch !=None) and (jsonSearch['display'] !=0)) :\n",
    "        for post in jsonSearch['items'] :\n",
    "            getPostData(post, jsonResult)\n",
    "        \n",
    "        nStart = jsonSearch['start'] + jsonSearch['display']\n",
    "        jsonSearch = getNaverSearchResult(sNode, search_text, nStart, display_count)\n",
    "        \n",
    "    with open('%s_naver_%s.json' % (search_text, sNode), 'w', encoding='utf8') as outfile :\n",
    "        retJson = json.dumps(jsonResult, indent = 4, sort_keys = True, ensure_ascii=False)\n",
    "        outfile.write(retJson)   \n",
    "    print('%s_naver_%s.json SAVED' % (search_text, sNode))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' :\n",
    "    main(name1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sNode = 'blog'\n",
    "name_dict={}\n",
    "for n,i in enumerate(name1) :\n",
    "    if '+' in i :\n",
    "        name_dict[i.split('+')[1].split(' ')[0]] = n\n",
    "    else :\n",
    "        name_dict[i] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(name_len) :\n",
    "    a = open(name1[i]+'_naver_blog.json','rt', encoding='UTF8')\n",
    "    json1.append(json.load(a))\n",
    "a.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blog이름에 컨설팅, 창업이 있는 description 제거\n",
    "\n",
    "for i in range(0,11) :\n",
    "    for j in range(len(json1[i])) :\n",
    "        if ('컨설팅' in json1[i][j]['blogname']) | ('창업' in json1[i][j]['blogname']) :\n",
    "            continue\n",
    "        else :\n",
    "            des.append(json1[i][j]['description'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des1=[i.replace('<b>','').replace('</b>','') for i in des]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(des1).to_csv('lunch_project2',encoding='UTF8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blog이름에 컨설팅, 창업이 있는 description 제거\n",
    "# 각 검색어의 검색결과 개수 계산\n",
    "sum=0\n",
    "suma=[]\n",
    "for i in range(0,11) :\n",
    "    for j in range(len(json1[i])) :\n",
    "        if ('컨설팅' in json1[i][j]['blogname']) | ('창업' in json1[i][j]['blogname']) :\n",
    "            continue\n",
    "        else :\n",
    "            sum+=1\n",
    "    suma.append(sum)\n",
    "    sum=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1=0\n",
    "\n",
    "temp1=[]\n",
    "des2=[]\n",
    "for i in range(11) :\n",
    "    for j in range(suma[i]) :\n",
    "        temp1.append(des1[sum1+j])\n",
    "    des2.append(list(np.unique(temp1)))\n",
    "    temp1=[]\n",
    "    sum1+=suma[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 검색어의 결과 위치\n",
    "suma2=[len(i) for i in des2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#okt 사용 토큰화\n",
    "import nltk\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(words) :\n",
    "    t = Okt()\n",
    "    word_dic = {}\n",
    "    malist = t.pos(words, stem=True)\n",
    "    for word in malist :\n",
    "        if word[1] in ['Noun', 'Verb', 'Adjective'] :\n",
    "            word_dic[word[0]] = word_dic.get(word[0], 0) + 1\n",
    "    return word_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopword 지정 - notcounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notcounting=['장애인','사업', '컨설팅', '생산', '매물', '우엉갈비', '캐드', '거가대교', '주부', '승마장', '공화국', '전남대', '방위비', '횟집', '창업','JAVA', '이마트', '종가집', '곽지민', '배달' ,'건축', '건설', '화장품', '인쇄', '보호', '작업장','에어컨', '문화', '섬유', '학교', '법인', '비밀넷', '콘트롤', '태권도', '어린이집', '예은', '휴대폰', '일본']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict0=token_count(des2[0])\n",
    "keys = sorted(word_dict0.items(), key = lambda x : x[1], reverse = True)\n",
    "for word, count in keys :\n",
    "    print(\"{}({})\".format(word, count), end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 워드클라우드 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Affinity\\.conda\\envs\\keras\\lib\\site-packages\\jpype\\_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'des3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e746bd70333d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOkt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mmalist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdes3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mtowordcl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmalist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnotcounting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'des3' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from konlpy.corpus import kolaw\n",
    "from konlpy.tag import Okt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "\n",
    "def token_count(words) :\n",
    "    t = Okt()\n",
    "    word_dic = {}\n",
    "    malist = t.pos(words, stem=True)\n",
    "    \n",
    "def towordcl(worddict, notcounting=[]) :\n",
    "    text1=[]\n",
    "    aa=''\n",
    "    for i in worddict :\n",
    "        if i[1] in ['Noun', 'Verb', 'Adjective'] :\n",
    "            text1.append(i[0])\n",
    "    tokens_ko = [each_word for each_word in text1 if each_word not in notcounting]\n",
    "    sel_word = nltk.Text(tokens_ko) # nltk의 Text형태로 묶는..?\n",
    "    data = sel_word.vocab().most_common(1000)          \n",
    "    tmp_data = dict(data)\n",
    "    wordcloud=WordCloud(font_path = \"C:/Windows/Fonts/YBUU02.ttf\", background_color='white')\n",
    "    wordcloud=wordcloud.generate_from_frequencies(tmp_data)\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "for i in range(11) :\n",
    "    t=Okt()\n",
    "    malist = t.pos(des3[i], stem=True)\n",
    "    towordcl(malist, notcounting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-89ba4639baab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#dataset 모양 정리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0munch2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lunch_final.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cp949'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlunch2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#dataset 모양 정리\n",
    "unch2=pd.read_csv('lunch_final.csv',encoding = 'cp949')\n",
    "lunch2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>김밥천국</th>\n",
       "      <th>포베이</th>\n",
       "      <th>삼백집</th>\n",
       "      <th>밥뜨랑</th>\n",
       "      <th>다원</th>\n",
       "      <th>한려수도</th>\n",
       "      <th>봉태민</th>\n",
       "      <th>종로김밥</th>\n",
       "      <th>명동칼국수</th>\n",
       "      <th>돈토</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    김밥천국  포베이  삼백집  밥뜨랑  다원  한려수도  봉태민  종로김밥  명동칼국수  돈토\n",
       "0      3    4    3    3   3     5    3     3      4   3\n",
       "1      2    4    4    3   4     3    5     4      4   3\n",
       "2      1    2    4    3   3     4    3     1      3   3\n",
       "3      4    5    5    5   5     5    5     4      5   4\n",
       "4      2    4    3    2   4     5    3     3      5   4\n",
       "5      3    3    2    3   4     4    3     4      4   4\n",
       "6      4    4    3    5   5     5    5     3      2   4\n",
       "7      3    4    2    5   4     4    4     2      3   4\n",
       "8      2    4    4    3   3     4    5     3      5   3\n",
       "9      1    4    3    2   5     1    5     2      4   5\n",
       "10     2    2    1    5   5     5    3     1      2   2\n",
       "11     2    3    2    5   4     4    4     2      3   4\n",
       "12     1    3    1    5   3     3    4     1      4   4\n",
       "13     3    4    3    3   3     5    3     2      4   3\n",
       "14     2    3    1    5   4     2    5     3      4   5\n",
       "15     4    2    3    4   4     4    3     1      2   4\n",
       "16     3    3    3    4   4     3    4     3      3   4\n",
       "17     1    4    1    4   1     4    1     1      4   4\n",
       "18     4    3    3    4   4     3    3     2      3   4\n",
       "19     2    4    3    3   3     5    4     3      4   3\n",
       "20     3    3    3    3   3     4    3     3      4   4\n",
       "21     3    2    5    2   3     3    5     3      4   3\n",
       "22     4    4    3    5   5     5    5     3      2   4\n",
       "23     3    4    3    2   5     4    4     3      5   3\n",
       "24     2    4    4    3   4     3    5     4      4   3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lun_daf=lunch2.iloc[:,1:11][:25]\n",
    "lun_daf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lunda.columns=['김밥천국','포보스','삼백집','밥뜨랑','다원','한려수도','봉태민','종로김밥','명동칼국수','돈토']\n",
    "lun_daf.index=lunch2['이름'][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lun_daf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=[]\n",
    "for i in range(25):\n",
    "        m=(np.ones(10)*i)\n",
    "        m=list(m)\n",
    "        user.append(m)\n",
    "\n",
    "item=[]\n",
    "p=np.arange(10)\n",
    "for i in range(25):\n",
    "    c=list(p)\n",
    "    item.append(p)\n",
    "\n",
    "item=np.array(item).reshape(250,)\n",
    "user=np.array(user).reshape(250,)\n",
    "rating=np.array(lun_daf).astype('int').reshape(250,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "da=np.concatenate((user.astype('str'),item,rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "da=da.reshape([3,-1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "da2=pd.DataFrame(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "da2.iloc[:,2] = [int(i) for i in da[:,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 3 columns):\n",
      "0    250 non-null object\n",
      "1    250 non-null object\n",
      "2    250 non-null int64\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 5.9+ KB\n"
     ]
    }
   ],
   "source": [
    "da2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.0', '0', 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(da2.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=[]\n",
    "for i in range(250):\n",
    "    t=tuple(da2.iloc[i,:])\n",
    "    dat.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# surprise의 dataset으로 만들어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Affinity\\.conda\\envs\\keras\\lib\\site-packages\\scikit_surprise-1.0.6-py3.6-win-amd64.egg\\surprise\\dataset.py:61: UserWarning: Using rating_scale from reader, deprecated. Set rating_scale at dataset creation instead (load_from_file, load_from_folds, or load_from_df).\n",
      "  warnings.warn('Using rating_scale from reader, deprecated. Set '\n"
     ]
    }
   ],
   "source": [
    "from surprise import NormalPredictor\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "df = pd.DataFrame(dat, columns=['user','item','rating'])\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "lundata= surprise.dataset.Dataset.load_from_df(df,reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline - KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsl_param={'method':'als','n_epochs':5, 'reg_u':7, 'reg_i':3}\n",
    "bsl_param2={'method':'sgd','reg':0.02, 'learning_rate':0.005, 'n_epochs':10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 알고리즘 \n",
    ": sgd(stochastic Gradient Descent) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n",
      "MAE:  0.8354\n",
      "Estimating biases using sgd...\n",
      "MAE:  0.8213\n",
      "Estimating biases using sgd...\n",
      "MAE:  0.7744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8103944605580172"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model= surprise.BaselineOnly(bsl_options=bsl_param2)\n",
    "\n",
    "acc=np.zeros(3)\n",
    "cv=KFold(3)\n",
    "\n",
    "for i ,(trainset, testset) in enumerate(cv.split(lundata)):\n",
    "    model.fit(trainset)\n",
    "    pred=model.test(testset)\n",
    "    acc[i] = surprise.accuracy.mae(pred, verbose=True)\n",
    "acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":Als(alternating Least Squares) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "MAE:  0.7840\n",
      "Estimating biases using als...\n",
      "MAE:  0.8352\n",
      "Estimating biases using als...\n",
      "MAE:  0.7755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.798215706475471"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model= surprise.BaselineOnly(bsl_options=bsl_param)\n",
    "\n",
    "acc=np.zeros(3)\n",
    "cv=KFold(3)\n",
    "\n",
    "for i ,(trainset, testset) in enumerate(cv.split(lundata)):\n",
    "    model.fit(trainset)\n",
    "    pred=model.test(testset)\n",
    "    acc[i] = surprise.accuracy.mae(pred, verbose=True)\n",
    "acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline - cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "Estimating biases using sgd...\n",
      "MAE:  0.7755\n"
     ]
    }
   ],
   "source": [
    "# : sgd(stochastic Gradient Descent) 사용\n",
    "np.random.seed(1)\n",
    "\n",
    "model2= surprise.BaselineOnly(bsl_options=bsl_param2)\n",
    "cross_validate(model2, lundata)\n",
    "acc = surprise.accuracy.mae(pred, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "MAE:  0.7755\n"
     ]
    }
   ],
   "source": [
    "#:Als(alternating Least Squares) 사용\n",
    "np.random.seed(1)\n",
    "\n",
    "model2= surprise.BaselineOnly(bsl_options=bsl_param)\n",
    "cross_validate(model2, lundata)\n",
    "acc = surprise.accuracy.mae(pred, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이웃 기반 모형\n",
    ": 유사도 기반"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. msd : 평균제곱차이 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7930861289915383"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "sim_options = {'name':'msd'}\n",
    "algo= surprise.KNNBasic(sim_options = sim_options)\n",
    "cross_validate(algo, lundata)[\"test_mae\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cosine 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8220529014486699"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "sim_options = {'name':'cosine'}\n",
    "algo= surprise.KNNBasic(sim_options = sim_options)\n",
    "cross_validate(algo, lundata)[\"test_mae\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pearson 유사도\n",
    ": 두 벡터의 상관 계수를 -1~1의 수로 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7781985364593123"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "sim_options = {'name':'pearson'}\n",
    "algo= surprise.KNNBasic(sim_options = sim_options)\n",
    "cross_validate(algo, lundata)[\"test_mae\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pearson_baseline 유사도\n",
    ": 각 벡터의 기대값을 단순 평균이 아니라 베이스라인 모형에서 예측한 값을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7520818728670766"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "sim_options = {'name':'pearson_baseline'}\n",
    "algo= surprise.KNNBasic(sim_options = sim_options)\n",
    "cross_validate(algo, lundata)[\"test_mae\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. matrix Factorization\n",
    ": 모든 사용자와 상품에 대해 오차 함수를 최소화하는 요인 벡터를 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8038445606836927"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "algo= surprise.SVD(n_factors=50)\n",
    "cross_validate(algo, lundata)[\"test_mae\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVDpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.792798991509627"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "algo= surprise.SVDpp(n_factors=70)\n",
    "cross_validate(algo, lundata)[\"test_mae\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF(Non-negative matrix factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7796614502801005"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "algo= surprise.NMF(n_factors=100)\n",
    "cross_validate(algo, lundata)[\"test_mae\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# surprise의 dataset으로 만들어주기2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=[]\n",
    "for i in range(25):\n",
    "        m=(np.ones(10)*i)\n",
    "        m=list(m)\n",
    "        user.append(m)\n",
    "\n",
    "item=[]\n",
    "p=np.arange(10)\n",
    "for i in range(25):\n",
    "    c=list(p)\n",
    "    item.append(p)\n",
    "\n",
    "item=np.array(item).reshape(250,)\n",
    "user=np.array(user).reshape(250,)\n",
    "rating=np.array(lun_daf).reshape(250,)\n",
    "\n",
    "# da=np.concatenate((user,item,rating))\n",
    "# da=da.reshape([3,-1]).T #zip 한줄로 가능\n",
    "da=list(zip(user,item,rating))\n",
    "\n",
    "dat2=[] # tuple안에 list 형태로 되어있어서 형태를 만들어줌\n",
    "for i in range(250):\n",
    "    t=tuple(da[i])\n",
    "    dat2.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NormalPredictor\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "df2 = pd.DataFrame(dat2, columns=['user','item','rating'])\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "lundata2= surprise.dataset.Dataset.load_from_df(df2,reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0, 3.0, None),\n",
       " (0.0, 1.0, 4.0, None),\n",
       " (0.0, 2.0, 3.0, None),\n",
       " (0.0, 3.0, 3.0, None),\n",
       " (0.0, 4.0, 3.0, None),\n",
       " (0.0, 5.0, 5.0, None),\n",
       " (0.0, 6.0, 3.0, None),\n",
       " (0.0, 7.0, 3.0, None),\n",
       " (0.0, 8.0, 4.0, None),\n",
       " (0.0, 9.0, 3.0, None),\n",
       " (1.0, 0.0, 2.0, None),\n",
       " (1.0, 1.0, 4.0, None),\n",
       " (1.0, 2.0, 4.0, None),\n",
       " (1.0, 3.0, 3.0, None),\n",
       " (1.0, 4.0, 4.0, None),\n",
       " (1.0, 5.0, 3.0, None),\n",
       " (1.0, 6.0, 5.0, None),\n",
       " (1.0, 7.0, 4.0, None),\n",
       " (1.0, 8.0, 4.0, None),\n",
       " (1.0, 9.0, 3.0, None),\n",
       " (2.0, 0.0, 1.0, None),\n",
       " (2.0, 1.0, 2.0, None),\n",
       " (2.0, 2.0, 4.0, None),\n",
       " (2.0, 3.0, 3.0, None),\n",
       " (2.0, 4.0, 3.0, None),\n",
       " (2.0, 5.0, 4.0, None),\n",
       " (2.0, 6.0, 3.0, None),\n",
       " (2.0, 7.0, 1.0, None),\n",
       " (2.0, 8.0, 3.0, None),\n",
       " (2.0, 9.0, 3.0, None),\n",
       " (3.0, 0.0, 4.0, None),\n",
       " (3.0, 1.0, 5.0, None),\n",
       " (3.0, 2.0, 5.0, None),\n",
       " (3.0, 3.0, 5.0, None),\n",
       " (3.0, 4.0, 5.0, None),\n",
       " (3.0, 5.0, 5.0, None),\n",
       " (3.0, 6.0, 5.0, None),\n",
       " (3.0, 7.0, 4.0, None),\n",
       " (3.0, 8.0, 5.0, None),\n",
       " (3.0, 9.0, 4.0, None),\n",
       " (4.0, 0.0, 2.0, None),\n",
       " (4.0, 1.0, 4.0, None),\n",
       " (4.0, 2.0, 3.0, None),\n",
       " (4.0, 3.0, 2.0, None),\n",
       " (4.0, 4.0, 4.0, None),\n",
       " (4.0, 5.0, 5.0, None),\n",
       " (4.0, 6.0, 3.0, None),\n",
       " (4.0, 7.0, 3.0, None),\n",
       " (4.0, 8.0, 5.0, None),\n",
       " (4.0, 9.0, 4.0, None),\n",
       " (5.0, 0.0, 3.0, None),\n",
       " (5.0, 1.0, 3.0, None),\n",
       " (5.0, 2.0, 2.0, None),\n",
       " (5.0, 3.0, 3.0, None),\n",
       " (5.0, 4.0, 4.0, None),\n",
       " (5.0, 5.0, 4.0, None),\n",
       " (5.0, 6.0, 3.0, None),\n",
       " (5.0, 7.0, 4.0, None),\n",
       " (5.0, 8.0, 4.0, None),\n",
       " (5.0, 9.0, 4.0, None),\n",
       " (6.0, 0.0, 4.0, None),\n",
       " (6.0, 1.0, 4.0, None),\n",
       " (6.0, 2.0, 3.0, None),\n",
       " (6.0, 3.0, 5.0, None),\n",
       " (6.0, 4.0, 5.0, None),\n",
       " (6.0, 5.0, 5.0, None),\n",
       " (6.0, 6.0, 5.0, None),\n",
       " (6.0, 7.0, 3.0, None),\n",
       " (6.0, 8.0, 2.0, None),\n",
       " (6.0, 9.0, 4.0, None),\n",
       " (7.0, 0.0, 3.0, None),\n",
       " (7.0, 1.0, 4.0, None),\n",
       " (7.0, 2.0, 2.0, None),\n",
       " (7.0, 3.0, 5.0, None),\n",
       " (7.0, 4.0, 4.0, None),\n",
       " (7.0, 5.0, 4.0, None),\n",
       " (7.0, 6.0, 4.0, None),\n",
       " (7.0, 7.0, 2.0, None),\n",
       " (7.0, 8.0, 3.0, None),\n",
       " (7.0, 9.0, 4.0, None),\n",
       " (8.0, 0.0, 2.0, None),\n",
       " (8.0, 1.0, 4.0, None),\n",
       " (8.0, 2.0, 4.0, None),\n",
       " (8.0, 3.0, 3.0, None),\n",
       " (8.0, 4.0, 3.0, None),\n",
       " (8.0, 5.0, 4.0, None),\n",
       " (8.0, 6.0, 5.0, None),\n",
       " (8.0, 7.0, 3.0, None),\n",
       " (8.0, 8.0, 5.0, None),\n",
       " (8.0, 9.0, 3.0, None),\n",
       " (9.0, 0.0, 1.0, None),\n",
       " (9.0, 1.0, 4.0, None),\n",
       " (9.0, 2.0, 3.0, None),\n",
       " (9.0, 3.0, 2.0, None),\n",
       " (9.0, 4.0, 5.0, None),\n",
       " (9.0, 5.0, 1.0, None),\n",
       " (9.0, 6.0, 5.0, None),\n",
       " (9.0, 7.0, 2.0, None),\n",
       " (9.0, 8.0, 4.0, None),\n",
       " (9.0, 9.0, 5.0, None),\n",
       " (10.0, 0.0, 2.0, None),\n",
       " (10.0, 1.0, 2.0, None),\n",
       " (10.0, 2.0, 1.0, None),\n",
       " (10.0, 3.0, 5.0, None),\n",
       " (10.0, 4.0, 5.0, None),\n",
       " (10.0, 5.0, 5.0, None),\n",
       " (10.0, 6.0, 3.0, None),\n",
       " (10.0, 7.0, 1.0, None),\n",
       " (10.0, 8.0, 2.0, None),\n",
       " (10.0, 9.0, 2.0, None),\n",
       " (11.0, 0.0, 2.0, None),\n",
       " (11.0, 1.0, 3.0, None),\n",
       " (11.0, 2.0, 2.0, None),\n",
       " (11.0, 3.0, 5.0, None),\n",
       " (11.0, 4.0, 4.0, None),\n",
       " (11.0, 5.0, 4.0, None),\n",
       " (11.0, 6.0, 4.0, None),\n",
       " (11.0, 7.0, 2.0, None),\n",
       " (11.0, 8.0, 3.0, None),\n",
       " (11.0, 9.0, 4.0, None),\n",
       " (12.0, 0.0, 1.0, None),\n",
       " (12.0, 1.0, 3.0, None),\n",
       " (12.0, 2.0, 1.0, None),\n",
       " (12.0, 3.0, 5.0, None),\n",
       " (12.0, 4.0, 3.0, None),\n",
       " (12.0, 5.0, 3.0, None),\n",
       " (12.0, 6.0, 4.0, None),\n",
       " (12.0, 7.0, 1.0, None),\n",
       " (12.0, 8.0, 4.0, None),\n",
       " (12.0, 9.0, 4.0, None),\n",
       " (13.0, 0.0, 3.0, None),\n",
       " (13.0, 1.0, 4.0, None),\n",
       " (13.0, 2.0, 3.0, None),\n",
       " (13.0, 3.0, 3.0, None),\n",
       " (13.0, 4.0, 3.0, None),\n",
       " (13.0, 5.0, 5.0, None),\n",
       " (13.0, 6.0, 3.0, None),\n",
       " (13.0, 7.0, 2.0, None),\n",
       " (13.0, 8.0, 4.0, None),\n",
       " (13.0, 9.0, 3.0, None),\n",
       " (14.0, 0.0, 2.0, None),\n",
       " (14.0, 1.0, 3.0, None),\n",
       " (14.0, 2.0, 1.0, None),\n",
       " (14.0, 3.0, 5.0, None),\n",
       " (14.0, 4.0, 4.0, None),\n",
       " (14.0, 5.0, 2.0, None),\n",
       " (14.0, 6.0, 5.0, None),\n",
       " (14.0, 7.0, 3.0, None),\n",
       " (14.0, 8.0, 4.0, None),\n",
       " (14.0, 9.0, 5.0, None),\n",
       " (15.0, 0.0, 4.0, None),\n",
       " (15.0, 1.0, 2.0, None),\n",
       " (15.0, 2.0, 3.0, None),\n",
       " (15.0, 3.0, 4.0, None),\n",
       " (15.0, 4.0, 4.0, None),\n",
       " (15.0, 5.0, 4.0, None),\n",
       " (15.0, 6.0, 3.0, None),\n",
       " (15.0, 7.0, 1.0, None),\n",
       " (15.0, 8.0, 2.0, None),\n",
       " (15.0, 9.0, 4.0, None),\n",
       " (16.0, 0.0, 3.0, None),\n",
       " (16.0, 1.0, 3.0, None),\n",
       " (16.0, 2.0, 3.0, None),\n",
       " (16.0, 3.0, 4.0, None),\n",
       " (16.0, 4.0, 4.0, None),\n",
       " (16.0, 5.0, 3.0, None),\n",
       " (16.0, 6.0, 4.0, None),\n",
       " (16.0, 7.0, 3.0, None),\n",
       " (16.0, 8.0, 3.0, None),\n",
       " (16.0, 9.0, 4.0, None),\n",
       " (17.0, 0.0, 1.0, None),\n",
       " (17.0, 1.0, 4.0, None),\n",
       " (17.0, 2.0, 1.0, None),\n",
       " (17.0, 3.0, 4.0, None),\n",
       " (17.0, 4.0, 1.0, None),\n",
       " (17.0, 5.0, 4.0, None),\n",
       " (17.0, 6.0, 1.0, None),\n",
       " (17.0, 7.0, 1.0, None),\n",
       " (17.0, 8.0, 4.0, None),\n",
       " (17.0, 9.0, 4.0, None),\n",
       " (18.0, 0.0, 4.0, None),\n",
       " (18.0, 1.0, 3.0, None),\n",
       " (18.0, 2.0, 3.0, None),\n",
       " (18.0, 3.0, 4.0, None),\n",
       " (18.0, 4.0, 4.0, None),\n",
       " (18.0, 5.0, 3.0, None),\n",
       " (18.0, 6.0, 3.0, None),\n",
       " (18.0, 7.0, 2.0, None),\n",
       " (18.0, 8.0, 3.0, None),\n",
       " (18.0, 9.0, 4.0, None),\n",
       " (19.0, 0.0, 2.0, None),\n",
       " (19.0, 1.0, 4.0, None),\n",
       " (19.0, 2.0, 3.0, None),\n",
       " (19.0, 3.0, 3.0, None),\n",
       " (19.0, 4.0, 3.0, None),\n",
       " (19.0, 5.0, 5.0, None),\n",
       " (19.0, 6.0, 4.0, None),\n",
       " (19.0, 7.0, 3.0, None),\n",
       " (19.0, 8.0, 4.0, None),\n",
       " (19.0, 9.0, 3.0, None),\n",
       " (20.0, 0.0, 3.0, None),\n",
       " (20.0, 1.0, 3.0, None),\n",
       " (20.0, 2.0, 3.0, None),\n",
       " (20.0, 3.0, 3.0, None),\n",
       " (20.0, 4.0, 3.0, None),\n",
       " (20.0, 5.0, 4.0, None),\n",
       " (20.0, 6.0, 3.0, None),\n",
       " (20.0, 7.0, 3.0, None),\n",
       " (20.0, 8.0, 4.0, None),\n",
       " (20.0, 9.0, 4.0, None),\n",
       " (21.0, 0.0, 3.0, None),\n",
       " (21.0, 1.0, 2.0, None),\n",
       " (21.0, 2.0, 5.0, None),\n",
       " (21.0, 3.0, 2.0, None),\n",
       " (21.0, 4.0, 3.0, None),\n",
       " (21.0, 5.0, 3.0, None),\n",
       " (21.0, 6.0, 5.0, None),\n",
       " (21.0, 7.0, 3.0, None),\n",
       " (21.0, 8.0, 4.0, None),\n",
       " (21.0, 9.0, 3.0, None),\n",
       " (22.0, 0.0, 4.0, None),\n",
       " (22.0, 1.0, 4.0, None),\n",
       " (22.0, 2.0, 3.0, None),\n",
       " (22.0, 3.0, 5.0, None),\n",
       " (22.0, 4.0, 5.0, None),\n",
       " (22.0, 5.0, 5.0, None),\n",
       " (22.0, 6.0, 5.0, None),\n",
       " (22.0, 7.0, 3.0, None),\n",
       " (22.0, 8.0, 2.0, None),\n",
       " (22.0, 9.0, 4.0, None),\n",
       " (23.0, 0.0, 3.0, None),\n",
       " (23.0, 1.0, 4.0, None),\n",
       " (23.0, 2.0, 3.0, None),\n",
       " (23.0, 3.0, 2.0, None),\n",
       " (23.0, 4.0, 5.0, None),\n",
       " (23.0, 5.0, 4.0, None),\n",
       " (23.0, 6.0, 4.0, None),\n",
       " (23.0, 7.0, 3.0, None),\n",
       " (23.0, 8.0, 5.0, None),\n",
       " (23.0, 9.0, 3.0, None),\n",
       " (24.0, 0.0, 2.0, None),\n",
       " (24.0, 1.0, 4.0, None),\n",
       " (24.0, 2.0, 4.0, None),\n",
       " (24.0, 3.0, 3.0, None),\n",
       " (24.0, 4.0, 4.0, None),\n",
       " (24.0, 5.0, 3.0, None),\n",
       " (24.0, 6.0, 5.0, None),\n",
       " (24.0, 7.0, 4.0, None),\n",
       " (24.0, 8.0, 4.0, None),\n",
       " (24.0, 9.0, 3.0, None)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lundata2.raw_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.knns.KNNBasic at 0x1c559b2a898>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = lundata2\n",
    "\n",
    "# Retrieve the trainset.\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Build an algorithm, and train it.\n",
    "sim_options = {'name':'msd'}\n",
    "algo= surprise.KNNBasic(sim_options = sim_options)\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1=rating.copy()\n",
    "test1=test1.reshape((-1,10))\n",
    "test2=test1.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 2.0        item: 0          r_ui = 2.56   est = 2.44   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 2.0        item: 1          r_ui = 3.44   est = 3.28   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 2.0        item: 2          r_ui = 2.88   est = 2.96   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 2.0        item: 3          r_ui = 3.64   est = 3.52   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 2.0        item: 4          r_ui = 3.80   est = 3.65   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 2.0        item: 5          r_ui = 3.88   est = 3.91   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 2.0        item: 6          r_ui = 3.88   est = 3.73   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 2.0        item: 7          r_ui = 2.56   est = 2.40   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 2.0        item: 8          r_ui = 3.64   est = 3.59   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 2.0        item: 9          r_ui = 3.64   est = 3.55   {'actual_k': 25, 'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "user_no=2.0\n",
    "\n",
    "from surprise import PredictionImpossible\n",
    "ests=[]\n",
    "for i in range(10):\n",
    "    a= test2[i]\n",
    "    pred = algo.predict(user_no, i, r_ui= a, verbose=True)\n",
    "    ests.append(pred.est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict ={'김밥천국': 0,\n",
    " '포보스': 1,\n",
    " '삼백집': 2,\n",
    " '밥뜨랑': 3,\n",
    " '다원': 4,\n",
    " '한려수도': 5,\n",
    " '봉태민': 6,\n",
    " '종로김밥': 7,\n",
    " '명동칼국수': 8,\n",
    " '돈토': 9,\n",
    " '파리바게트': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "des6 = pd.read_csv('lunch_project1.csv', encoding='cp949')\n",
    "des6['restaurant']=name_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 17400)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(des6['words'])\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_sin = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(des6.index, index = des6['restaurant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rs(title, n=10, sim = cosine_sin):\n",
    "    idx = indices[title] #해당 영화의 index를 찾아줌\n",
    "    sim_score = list(enumerate(cosine_sin[idx])) # 순서대로 0,1,2..와 함께 해당 영화와 다른 영화간의 cosine_sin값이 튜플 형태로 들어감\n",
    "    sim_score = sorted(sim_score, key = lambda x : x[1], reverse = True) # 값에 따라 정렬해줌, 내림차순 정렬\n",
    "    sim_score = sim_score[1:(n+1)] # 자기 자신이 아닌(=1이므로 가장 큼) n개만큼 반환\n",
    "    rest_index = [i[0] for i in sim_score] # (idx, cosin_sin값)[0] --> 해당 영화의 인덱스 값 반환\n",
    "    return des6.restaurant.iloc[rest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 점수 확인용\n",
    "def rs2(title, n=10, sim = cosine_sin):\n",
    "    idx = indices[title] #해당 영화의 index를 찾아줌\n",
    "    sim_score = list(enumerate(cosine_sin[idx])) # 순서대로 0,1,2..와 함께 해당 영화와 다른 영화간의 cosine_sin값이 튜플 형태로 들어감\n",
    "    sim_score = sorted(sim_score, key = lambda x : x[1], reverse = True) # 값에 따라 정렬해줌, 내림차순 정렬\n",
    "    sim_score = sim_score[1:(n+1)] # 자기 자신이 아닌(=1이므로 가장 큼) n개만큼 반환\n",
    "    rest_index = [i[0] for i in sim_score] # (idx, cosin_sin값)[0] --> 해당 영화의 인덱스 값 반환\n",
    "    des7 = pd.DataFrame(des6.restaurant.iloc[rest_index])\n",
    "    des7['sim_score']=sim_score\n",
    "    return des7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김밥천국\n",
      "7      종로김밥\n",
      "2       삼백집\n",
      "6       봉태민\n",
      "8     명동칼국수\n",
      "3       밥뜨랑\n",
      "9        돈토\n",
      "4        다원\n",
      "5      한려수도\n",
      "1       포보스\n",
      "10    파리바게트\n",
      "Name: restaurant, dtype: object\n",
      "\n",
      "\n",
      "포보스\n",
      "4        다원\n",
      "5      한려수도\n",
      "8     명동칼국수\n",
      "9        돈토\n",
      "6       봉태민\n",
      "2       삼백집\n",
      "3       밥뜨랑\n",
      "0      김밥천국\n",
      "7      종로김밥\n",
      "10    파리바게트\n",
      "Name: restaurant, dtype: object\n",
      "\n",
      "\n",
      "삼백집\n",
      "5      한려수도\n",
      "4        다원\n",
      "9        돈토\n",
      "8     명동칼국수\n",
      "6       봉태민\n",
      "0      김밥천국\n",
      "1       포보스\n",
      "7      종로김밥\n",
      "3       밥뜨랑\n",
      "10    파리바게트\n",
      "Name: restaurant, dtype: object\n",
      "\n",
      "\n",
      "밥뜨랑\n",
      "9        돈토\n",
      "4        다원\n",
      "5      한려수도\n",
      "6       봉태민\n",
      "8     명동칼국수\n",
      "1       포보스\n",
      "2       삼백집\n",
      "0      김밥천국\n",
      "7      종로김밥\n",
      "10    파리바게트\n",
      "Name: restaurant, dtype: object\n",
      "\n",
      "\n",
      "다원\n",
      "5      한려수도\n",
      "1       포보스\n",
      "9        돈토\n",
      "8     명동칼국수\n",
      "6       봉태민\n",
      "3       밥뜨랑\n",
      "2       삼백집\n",
      "0      김밥천국\n",
      "7      종로김밥\n",
      "10    파리바게트\n",
      "Name: restaurant, dtype: object\n",
      "\n",
      "\n",
      "한려수도\n",
      "4        다원\n",
      "9        돈토\n",
      "1       포보스\n",
      "8     명동칼국수\n",
      "2       삼백집\n",
      "6       봉태민\n",
      "3       밥뜨랑\n",
      "0      김밥천국\n",
      "7      종로김밥\n",
      "10    파리바게트\n",
      "Name: restaurant, dtype: object\n",
      "\n",
      "\n",
      "봉태민\n",
      "4        다원\n",
      "9        돈토\n",
      "5      한려수도\n",
      "8     명동칼국수\n",
      "3       밥뜨랑\n",
      "1       포보스\n",
      "2       삼백집\n",
      "0      김밥천국\n",
      "7      종로김밥\n",
      "10    파리바게트\n",
      "Name: restaurant, dtype: object\n",
      "\n",
      "\n",
      "종로김밥\n",
      "0      김밥천국\n",
      "6       봉태민\n",
      "2       삼백집\n",
      "4        다원\n",
      "9        돈토\n",
      "3       밥뜨랑\n",
      "8     명동칼국수\n",
      "5      한려수도\n",
      "1       포보스\n",
      "10    파리바게트\n",
      "Name: restaurant, dtype: object\n",
      "\n",
      "\n",
      "명동칼국수\n",
      "4        다원\n",
      "5      한려수도\n",
      "1       포보스\n",
      "9        돈토\n",
      "6       봉태민\n",
      "2       삼백집\n",
      "3       밥뜨랑\n",
      "0      김밥천국\n",
      "7      종로김밥\n",
      "10    파리바게트\n",
      "Name: restaurant, dtype: object\n",
      "\n",
      "\n",
      "돈토\n",
      "4        다원\n",
      "5      한려수도\n",
      "1       포보스\n",
      "8     명동칼국수\n",
      "6       봉태민\n",
      "3       밥뜨랑\n",
      "2       삼백집\n",
      "0      김밥천국\n",
      "7      종로김밥\n",
      "10    파리바게트\n",
      "Name: restaurant, dtype: object\n",
      "\n",
      "\n",
      "파리바게트\n",
      "0     김밥천국\n",
      "2      삼백집\n",
      "4       다원\n",
      "7     종로김밥\n",
      "3      밥뜨랑\n",
      "6      봉태민\n",
      "9       돈토\n",
      "8    명동칼국수\n",
      "1      포보스\n",
      "5     한려수도\n",
      "Name: restaurant, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in name_dict.keys() :\n",
    "    print(i)\n",
    "    print(rs(i))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 프린트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 0.0        item: 0          r_ui = 3.00   est = 2.63   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 0.0        item: 1          r_ui = 4.00   est = 3.52   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 0.0        item: 2          r_ui = 3.00   est = 2.95   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 0.0        item: 3          r_ui = 3.00   est = 3.44   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 0.0        item: 4          r_ui = 3.00   est = 3.68   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 0.0        item: 5          r_ui = 5.00   est = 4.08   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 0.0        item: 6          r_ui = 3.00   est = 3.75   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 0.0        item: 7          r_ui = 3.00   est = 2.68   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 0.0        item: 8          r_ui = 4.00   est = 3.76   {'actual_k': 25, 'was_impossible': False}\n",
      "user: 0.0        item: 9          r_ui = 3.00   est = 3.54   {'actual_k': 25, 'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "user_no=0.0\n",
    "\n",
    "from surprise import PredictionImpossible\n",
    "ests=[]\n",
    "for i in range(10):\n",
    "    a= rating[i]\n",
    "    pred = algo.predict(user_no, i, r_ui= a, verbose=True)\n",
    "    ests.append(pred.est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " user id : 0.0  을 위한 추천 식당 :  한려수도 \n",
      "비슷한 식당 : \n",
      " 4    다원\n",
      "9    돈토\n",
      "Name: restaurant, dtype: object\n"
     ]
    }
   ],
   "source": [
    "a=ests.index(max(ests))\n",
    "user_no=0.0\n",
    "\n",
    "for rest, num in name_dict.items():\n",
    "    if num == a:\n",
    "        print(\" user id :\", user_no,\" 을 위한 추천 식당 : \", rest, \"\\n비슷한 식당 : \\n\",rs(a)[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
